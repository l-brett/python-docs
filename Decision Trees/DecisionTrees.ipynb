{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ae7088",
   "metadata": {},
   "source": [
    "Sklearn describes decision trees as non-parametric supervised learning methods used in classification and regression. It offers a DecisionTreeClassifier method. See the sample code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46422ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree \n",
    "X = [[0, 0], [1, 1]] \n",
    "Y = [0, 1] \n",
    "clf = tree.DecisionTreeClassifier() \n",
    "clf = clf.fit(X, Y) \n",
    "clf.predict([[2., 2.]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4429c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [2, 2]] \n",
    "y = [0.5, 2.5] \n",
    "clf = tree.DecisionTreeRegressor() \n",
    "clf = clf.fit(X, y)\n",
    "clf.predict([[1, 1]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35940118",
   "metadata": {},
   "source": [
    "## Random forest \n",
    "\n",
    "Sklearn describes random forest as ‘a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting’.\n",
    "\n",
    "The sklearn ensemble library includes both RandomForestClassifier and RandomForestRegressor methods. Hyper-parameters can be set like number of estimators, split criterion, and maximum depth among many others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ee251",
   "metadata": {},
   "source": [
    "## Gradient boosting  \n",
    "The Sklearn ensemble library also offers gradient boosting methods, including:\n",
    "\n",
    "- GradientBoostingClassifier. \n",
    "\n",
    "- GradientBoosting. \n",
    "\n",
    "- HistGradientBoostingClassifier. \n",
    "\n",
    "- HistGradientBoostingRegressor.\n",
    "\n",
    "Gradient boosting algorithms are usually based on decision trees. Sklearn states that the HGBM (Histogram Gradient Boosting Model) ‘uses gradient boosting to iteratively improve the model’s performance by fitting each tree to the negative gradient of the loss function with respect to the predicted value’, and that HGBM ‘may be one of the most useful supervised learning models in scikit-learn’.\n",
    "\n",
    "HGBM is an optimisation of standard gradient boosting. Values are grouped into bins. Instead of using individual feature values, calculations are based on aggregates of the binned values. This reduces the computational requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
